{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO2+WZyAG8ibIdsnssjSLbn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulToronto/DataCamp---Introduction-to-Deep-Learning-with-Keras/blob/main/Part_3_of_4_Improving_Your_Model_Performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 of 4 - Improving Your Model Performance"
      ],
      "metadata": {
        "id": "JOyadaxZcGA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "URqUavAb1rX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "CBVcS9Fm1qxa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning curves\n",
        "\n",
        "- Learning curves provide a lot of information about your model\n",
        "- So far, we've seen two types:\n",
        "    1. Loss curve\n",
        "        - <img src='https://raw.githubusercontent.com/PaulToronto/DataCamp---Introduction-to-Deep-Learning-with-Keras/main/images/loss_curve.png'/>\n",
        "        - tends to decrease as epochs go by\n",
        "        - this is expected since our model is learning to minimize loss\n",
        "        - afer a certain number of epochs, the value converges and we've arrived at a minimum\n",
        "    2. Accuracy curve\n",
        "        - <img src='https://raw.githubusercontent.com/PaulToronto/DataCamp---Introduction-to-Deep-Learning-with-Keras/main/images/accuracy_curve.png'/>\n",
        "        - accuracy tends to increase as epochs go by\n",
        "        - this shows that the model makes fewer mistakes as it learns\n",
        "- Overfitting\n",
        "    - <img src='https://raw.githubusercontent.com/PaulToronto/DataCamp---Introduction-to-Deep-Learning-with-Keras/main/images/overfitting.png'/>\n",
        "    - Overfitting is when our model starts learning particularities of our training data which don't generalize well on unseen data\n",
        "        - The early stopping callback is useful to stop our model before it starts overfitting\n",
        "    - If we plot training vs validation data we can identify overfitting\n",
        "    - The training and validation curves start to diverge\n",
        "- Unstable curves\n",
        "    - <img src='https://raw.githubusercontent.com/PaulToronto/DataCamp---Introduction-to-Deep-Learning-with-Keras/main/images/unstable_curve.png'/>\n",
        "    - Not all curves are smooth and pretty\n",
        "    - There are many reasons that can lead to unstable learning curves:\n",
        "        - the chosen optimizer\n",
        "        - learning rate\n",
        "        - batch size\n",
        "        - network architecture\n",
        "        - weight initialization\n",
        "        - ...\n",
        "    - All these parameters can be tuned\n",
        "- Can we benefit from more data?\n",
        "    - Neural networks are well known for surpassing traditional machine learning techniquss as the amount of data increases\n",
        "    - We can check whether collecting more data would increase a model's generalization and accuracy     \n",
        "- We aim at producing a graph like this one, where we have fitted our model with increasing amounts of training data\n",
        "    - <img src='https://raw.githubusercontent.com/PaulToronto/DataCamp---Introduction-to-Deep-Learning-with-Keras/main/images/aim.png'/>\n",
        "    - If, after using all our data, we see that our test still has a tendency to improve (meaning it's not parallet to our training set curve and it's increasing), then it's worth it to gather more data if possible to allow the model to keep learning\n",
        "        - <img src='https://raw.githubusercontent.com/PaulToronto/DataCamp---Introduction-to-Deep-Learning-with-Keras/main/images/still_increasing.png'/>\n",
        "- Code for creating a graph like the previous one:\n",
        "\n",
        "```python\n",
        "# Store initial model weights\n",
        "init_weights = model.get_weights()\n",
        "\n",
        "# Lists for storing accuracies\n",
        "train_accs = []\n",
        "tests_accs = []\n",
        "\n",
        "# loop over a predetermined list of train sized and\n",
        "# for each train size we get the corresponding training data fraction\n",
        "for train_size in train_sizes:\n",
        "    # Split a fraction according to train_size\n",
        "    X_train_frac, _, y_train_frac, _ = train_test_split(X_train,\n",
        "                                                        y_train,\n",
        "                                                        train_size=train_size)\n",
        "    # make sure our model starts with the same set of weights\n",
        "    model.set_weights(initial_weights)\n",
        "    model.fit(X_train_frac,\n",
        "              y_train_frac,\n",
        "              epochs=100,\n",
        "              verbose=0,\n",
        "              callbacks=[EarlyStopping(monintor='loss', patience=1)])\n",
        "    # Get the accuracy for this training set fraction\n",
        "    train_acc = model.evaluate(X_train_frac, y_train_frac, verbose=0)[1]\n",
        "    train_accs.append(train_acc)\n",
        "    # Get the accuracy on the whole test set\n",
        "    test_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "    test_accs.append(test_acc)\n",
        "    print('Done with size: ', train_size)\n",
        "```\n"
      ],
      "metadata": {
        "id": "5_hOZhU9cNP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning the digits"
      ],
      "metadata": {
        "id": "P-yPZBQMFM-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Data\n",
        "\n",
        "The files `digits_pixels.npy` and `digits_target.npy` can be downloaded via these two links:\n",
        "\n",
        "https://github.com/PaulToronto/DataCamp---Introduction-to-Deep-Learning-with-Keras/raw/main/data/Digits/digits_pixels.npy\n",
        "\n",
        "https://github.com/PaulToronto/DataCamp---Introduction-to-Deep-Learning-with-Keras/raw/main/data/Digits/digits_target.npy\n",
        "\n",
        "I stored them in my Google Drive and I am reading them directly from there. I don't think it is possible to read *.npy files directly from GitHub because the files start downloading automatically when I call `np.load()`"
      ],
      "metadata": {
        "id": "-qXeZ7vK0V2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKfGN7Jr8MGt",
        "outputId": "4162103b-bc21-4a74-92a4-97cbd6727492"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/Colab Notebooks/Data Science/Mars-Seluna/'\n",
        "file_path += 'DataCamp - Introduction to Deep Learning with Keras/'\n",
        "file_path += 'digits_pixels.npy'\n",
        "\n",
        "X = np.load(file_path)\n",
        "\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/Data Science/Mars-Seluna/'\n",
        "file_path += 'DataCamp - Introduction to Deep Learning with Keras/'\n",
        "file_path += 'digits_target.npy'\n",
        "\n",
        "y = np.load(file_path)"
      ],
      "metadata": {
        "id": "udh6rLuD15Gr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEjDcSZRBBER",
        "outputId": "4dfbe114-59b4-4122-d978-84d65e5b4bf9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1797, 64),\n",
              " array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
              "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
              "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
              "        ...,\n",
              "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
              "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
              "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLKKb2i1CYew",
        "outputId": "4e5485ec-1733-4624-d871-4e47892756af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1797,), array([0, 1, 2, ..., 8, 9, 8]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCc_jPgpDJG9",
        "outputId": "737fbf6f-e154-43eb-9689-8c5cbe81c58a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1257, 64), (540, 64), (1257,), (540,))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build and compile the model"
      ],
      "metadata": {
        "id": "-nN2BqHkFS2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first, in the data section above, I need to one-hot encode the labels"
      ],
      "metadata": {
        "id": "uT5bs5UxEJ2i"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}